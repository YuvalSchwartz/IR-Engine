{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1R6oXqCxYQXIdcPcdf9wEykn177y0H0-C","timestamp":1672076742072}],"collapsed_sections":["b_nxf0-SzpYu","pdI05BaUBuLH","drgw3aH_XlaQ","jTXzoYPxCeSk","Bz9gGKptULwO","MCNoj5ZyA3ke","STtyI4OCqIoo","1yVgzbMfqhNj","fvZDJ7HQkFMf","qwN3_ba8rK4_","tfMJNQioA387"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **General Imports**"],"metadata":{"id":"jhRXKYhlqn8H"}},{"cell_type":"code","source":["import heapq\n","import math\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from timeit import timeit\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","import numpy as np\n","from google.cloud import storage\n","from contextlib import closing\n","from flask import Flask, request, jsonify\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHbOdYs0n8Gk","executionInfo":{"status":"ok","timestamp":1673609845506,"user_tz":-120,"elapsed":4889,"user":{"displayName":"Amit Kravchik","userId":"18309304432468869556"}},"outputId":"2b8d1436-6a5a-4e5b-8a60-ddacd6e72193"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["# **Spark's Initializing**"],"metadata":{"id":"_FNW1jf2rD_Y"}},{"cell_type":"code","source":["# These will already be installed in the testing environment so disregard the \n","# amount of time (~1 minute) it takes to install. \n","!pip install -q pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","!pip install -q graphframes\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n","spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n","!wget -N -P $spark_jars $graphframes_jar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6WHgQzXPoE_L","executionInfo":{"status":"ok","timestamp":1673609869177,"user_tz":-120,"elapsed":23682,"user":{"displayName":"Amit Kravchik","userId":"18309304432468869556"}},"outputId":"a1f46974-24f3-49de-83fd-ce0758e0eb44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["openjdk-8-jdk-headless is already the newest version (8u352-ga-1~18.04).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n","--2023-01-13 11:37:49--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n","Resolving repos.spark-packages.org (repos.spark-packages.org)... 54.230.18.120, 54.230.18.122, 54.230.18.61, ...\n","Connecting to repos.spark-packages.org (repos.spark-packages.org)|54.230.18.120|:443... connected.\n","HTTP request sent, awaiting response... 304 Not Modified\n","File ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ not modified on server. Omitting download.\n","\n"]}]},{"cell_type":"code","source":["import pyspark\n","# from pyspark.sql import *\n","# from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SQLContext, SparkSession\n","from pyspark.ml.feature import Tokenizer, RegexTokenizer\n","from graphframes import *\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType"],"metadata":{"id":"ELerdXZboHIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initializing spark context\n","# create a spark context and session\n","conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n","sc = pyspark.SparkContext(conf=conf)\n","sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n","spark = SparkSession.builder.getOrCreate()\n"],"metadata":{"id":"mz5Qn8HdoI6L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Building the InvertedIndex**"],"metadata":{"id":"lxl_J3eNqQuc"}},{"cell_type":"markdown","source":["## **MultiFileWriter + MultiFileReader Classes**"],"metadata":{"id":"b_nxf0-SzpYu"}},{"cell_type":"code","source":["BLOCK_SIZE = 1999998\n","\n","class MultiFileWriter:\n","    \"\"\" Sequential binary writer to multiple files of up to BLOCK_SIZE each. \"\"\"\n","    def __init__(self, base_dir, name):\n","        self._base_dir = Path(base_dir)\n","        self._base_dir.mkdir(parents=True, exist_ok=True)\n","        self._name = name\n","        self._file_gen = (open(self._base_dir / f'{name}_{i:03}.bin', 'wb') \n","                          for i in itertools.count())\n","        self._f = next(self._file_gen)\n","    \n","    def write(self, b):\n","      locs = []\n","      while len(b) > 0:\n","        pos = self._f.tell()\n","        remaining = BLOCK_SIZE - pos\n","        # if the current file is full, close and open a new one.\n","        if remaining == 0:  \n","          self._f.close()\n","          self._f = next(self._file_gen)\n","          pos, remaining = 0, BLOCK_SIZE\n","        self._f.write(b[:remaining])\n","        locs.append((self._f.name, pos))\n","        b = b[remaining:]\n","      return locs\n","\n","    def close(self):\n","      self._f.close()\n","\n","class MultiFileReader:\n","  \"\"\" Sequential binary reader of multiple files of up to BLOCK_SIZE each. \"\"\"\n","  def __init__(self):\n","    self._open_files = {}\n","\n","  def read(self, locs, n_bytes):\n","    b = []\n","    for f_name, offset in locs:\n","      if f_name not in self._open_files:\n","        self._open_files[f_name] = open(f_name, 'rb')\n","      f = self._open_files[f_name]\n","      f.seek(offset)\n","      n_read = min(n_bytes, BLOCK_SIZE - offset)\n","      b.append(f.read(n_read))\n","      n_bytes -= n_read\n","    return b''.join(b)\n","\n","  # def read_rdd(self, locs, n_bytes):\n","  #   sc = SparkContext.getOrCreate()\n","  #   b = []\n","  #   last_index_in_locs = len(locs) - 1\n","  #   last_file_name_in_locs = locs[-1][0]\n","  #   sum_bytes_readed = 0\n","  #   for index, filename_offset_tuple in enumerate(locs):\n","  #     if index != last_index_in_locs:\n","  #       sum_bytes_readed += BLOCK_SIZE - filename_offset_tuple[1]\n","  #   bytes_to_read_in_last_file = n_bytes - sum_bytes_readed\n","\n","  #   def read_file(file_name, offset):\n","  #     with open(file_name, 'rb') as f:\n","  #       f.seek(offset)\n","  #       if file_name == last_file_name_in_locs:\n","  #         return f.read(bytes_to_read_in_last_file)\n","  #       return f.read()\n","\n","  #   file_name_to_offset_rdd = sc.parallelize(locs)\n","  #   result = file_name_to_offset_rdd.map(lambda file_name_to_offset: read_file(*file_name_to_offset))\n","  #   return result\n","\n","  # def read_rdd(self, locs, n_bytes):\n","  #   sc = SparkContext.getOrCreate()\n","  #   b = []\n","  #   for f_name, offset in locs:\n","  #     if f_name not in self._open_files:\n","  #       self._open_files[f_name] = open(f_name, 'rb')\n","  #     f = self._open_files[f_name]\n","  #     f.seek(offset)\n","  #     n_read = min(n_bytes, BLOCK_SIZE - offset)\n","  #     b.append(f.read(n_read))\n","  #     n_bytes -= n_read\n","  #   return sc.parallelize(b)\n","  \n","  def close(self):\n","    for f in self._open_files.values():\n","      f.close()\n","\n","  def __exit__(self, exc_type, exc_value, traceback):\n","    self.close()\n","    return False"],"metadata":{"id":"detfV4MDzqd_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **InvertedIndex Class**"],"metadata":{"id":"b2StsOREyyZl"}},{"cell_type":"markdown","source":["### Class"],"metadata":{"id":"cDp65PZ6Lt8j"}},{"cell_type":"code","source":["TUPLE_SIZE = 6       # We're going to pack the doc_id and tf values in this \n","                     # many bytes.\n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n","\n","class InvertedIndex:  \n","  def __init__(self):\n","    \"\"\" Initializes the inverted index and add documents to it (if provided).\n","    Parameters:\n","    -----------\n","      docs: dict mapping doc_id to list of tokens\n","    \"\"\"\n","    # stores document frequency per term\n","    self.df = Counter()\n","    # stores total frequency per term\n","    self.term_total = Counter()\n","    # stores posting list per term while building the index (internally), \n","    # otherwise too big to store in memory.\n","    self._posting_list = defaultdict(list)\n","    # mapping a term to posting file locations, which is a list of \n","    # (file_name, offset) pairs. Since posting lists are big we are going to\n","    # write them to disk and just save their location in this list. We are \n","    # using the MultiFileWriter helper class to write fixed-size files and store\n","    # for each term/posting list its list of locations. The offset represents \n","    # the number of bytes from the beginning of the file where the posting list\n","    # starts. \n","    self.posting_locs = defaultdict(list)\n","    self.num_of_docs = 0\n","    # We're going to update and calculate this after each document. This will be usefull for the calculation of AVGDL (utilized in BM25) \n","    self.DL = defaultdict() \n","    self.doc_id_to_norm = defaultdict()\n","    self.doc_id_to_title = defaultdict()\n","    self.avg_dl = 0\n","\n","  def write_index(self, base_dir, name):\n","    \"\"\" Write the in-memory index to disk. Results in the file: \n","        (1) `name`.pkl containing the global term stats (e.g. df).\n","    \"\"\"\n","    self._write_globals(base_dir, name)\n","\n","  def _write_globals(self, base_dir, name):\n","    with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:\n","      pickle.dump(self, f)\n","\n","  def __getstate__(self):\n","    \"\"\" Modify how the object is pickled by removing the internal posting lists\n","        from the object's state dictionary. \n","    \"\"\"\n","    state = self.__dict__.copy()\n","    del state['_posting_list']\n","    return state\n","\n","  def posting_lists_iter(self):\n","    \"\"\" A generator that reads one posting list from disk and yields \n","        a (word:str, [(doc_id:int, tf:int), ...]) tuple.\n","    \"\"\"\n","    with closing(MultiFileReader()) as reader:\n","      for w, locs in self.posting_locs.items():\n","        b = reader.read(locs, self.df[w] * TUPLE_SIZE)\n","        posting_list = []\n","        for i in range(self.df[w]):\n","          doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","          tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","          posting_list.append((doc_id, tf))\n","        yield w, posting_list\n","\n","  @staticmethod\n","  def read_index(base_dir, name):\n","    with open(Path(base_dir) / f'{name}.pkl', 'rb') as f:\n","      return pickle.load(f)\n","\n","  @staticmethod\n","  def delete_index(base_dir, name):\n","    path_globals = Path(base_dir) / f'{name}.pkl'\n","    path_globals.unlink()\n","    for p in Path(base_dir).rglob(f'{name}_*.bin'):\n","      p.unlink()\n","\n","  @staticmethod\n","  def write_a_posting_list(b_w_pl,base_dir):\n","    ''' Takes a (bucket_id, [(w0, posting_list_0), (w1, posting_list_1), ...]) \n","    and writes it out to disk as files named {bucket_id}_XXX.bin under the \n","    current directory. Returns a posting locations dictionary that maps each \n","    word to the list of files and offsets that contain its posting list.\n","    Parameters:\n","    -----------\n","      b_w_pl: tuple\n","        Containing a bucket id and all (word, posting list) pairs in that bucket\n","        (bucket_id, [(w0, posting_list_0), (w1, posting_list_1), ...])\n","    Return:\n","      posting_locs: dict\n","        Posting locations for each of the words written out in this bucket.\n","    '''\n","    posting_locs = defaultdict(list)\n","    bucket, list_w_pl = b_w_pl\n","\n","    with closing(MultiFileWriter(base_dir, bucket)) as writer:\n","      for w, pl in list_w_pl: \n","        # convert to bytes\n","        b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n","                      for doc_id, tf in pl])\n","        # write to file(s)\n","        locs = writer.write(b)\n","      # save file locations to index\n","        posting_locs[w].extend(locs)\n","    return posting_locs\n","\n","  def read_posting_list(self, w, is_read_all):\n","    with closing(MultiFileReader()) as reader:\n","      locs = self.posting_locs[w]\n","      num_of_postings_to_read =  self.df[w] if is_read_all else min(K_TOP_ELEMENTS, self.df[w])\n","      b = reader.read(locs, num_of_postings_to_read * TUPLE_SIZE)\n","      posting_list = []\n","      for i in range(num_of_postings_to_read):\n","        doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","        tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","        posting_list.append((doc_id, tf))\n","      return posting_list\n","\n","  def read_posting_list_to_dict(self, w, is_read_all):\n","    with closing(MultiFileReader()) as reader:\n","      locs = self.posting_locs[w]\n","      num_of_postings_to_read =  self.df[w] if is_read_all else min(K_TOP_ELEMENTS, self.df[w])\n","      b = reader.read(locs, num_of_postings_to_read * TUPLE_SIZE)\n","      posting_list_dict = defaultdict()\n","      for i in range(num_of_postings_to_read):\n","        doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","        tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","        posting_list_dict[doc_id] = tf\n","      return posting_list_dict\n","\n","  # def read_posting_list_to_rdd(self, w):\n","  #   with closing(MultiFileReader()) as reader:\n","  #     locs = self.posting_locs[w]\n","  #     file_bytes_rdd = reader.read_rdd(locs, self.df[w] * TUPLE_SIZE)\n","  #     file_bytes_in_chunks_of_6_rdd = file_bytes_rdd.flatMap(lambda bytes_from_file: [bytes_from_file[i:i+6] for i in range(0, len(bytes_from_file), 6)])\n","  #     result = file_bytes_in_chunks_of_6_rdd.map(lambda six_bytes: (int.from_bytes(six_bytes[0:4], 'big'), int.from_bytes(six_bytes[4:6], 'big')))\n","  #     return result"],"metadata":{"id":"FRNib36xy_Ez"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cosine-Similarity methods in InvertedIndex class\n"],"metadata":{"id":"pdI05BaUBuLH"}},{"cell_type":"code","source":["class InvertedIndex(InvertedIndex):\n","  def generate_tfidf_scores_vectors(self, query_to_search, is_all_results):\n","    \"\"\" \n","    Parameters:\n","    -----------\n","    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","    index:           inverted index loaded from the corresponding files.    \n","    \n","    Returns:\n","    -----------\n","    docs_result: {key: doc_id, value: vector (list) of query words docs scores}\n","    query_result: vector (list) of query words scores\n","    \"\"\"\n","    epsilon = .0000001\n","    docs_result = defaultdict()\n","    query_counter = Counter(query_to_search)\n","    num_of_tokens_in_query = len(query_to_search)\n","    unique_query_tokens = set(query_to_search)\n","    num_of_unique_tokens_in_query = len(unique_query_tokens)\n","    index_and_query_tokens = enumerate(unique_query_tokens)\n","    query_result = [0] * num_of_unique_tokens_in_query\n","    for token_index, token in index_and_query_tokens:\n","      if token in self.df.keys(): #avoid terms that do not appear in the index.\n","        query_tf = query_counter[token] / num_of_tokens_in_query # term frequency divded by the length of the query\n","        index_idf = math.log(self.num_of_docs / (self.df[token] + epsilon), 10) #smoothing\n","            \n","        list_of_candidates_docs = self.read_posting_list(token, is_all_results)\n","        for doc_id, freq in list_of_candidates_docs:\n","          if doc_id not in docs_result:\n","            docs_result[doc_id] = [0] * num_of_unique_tokens_in_query\n","          docs_result[doc_id][token_index] = freq / self.DL[doc_id] * index_idf\n","            \n","        query_result[token_index] = query_tf * index_idf\n","    return docs_result, query_result\n","    \n","  # def generate_tfidf_scores_rdd_and_vector(self, query_to_search):\n","  #   sc = SparkContext.getOrCreate()\n","  #   epsilon = .0000001\n","  #   query_counter = Counter(query_to_search)\n","  #   num_of_tokens_in_query = len(query_to_search)\n","  #   unique_query_tokens = set(query_to_search)\n","  #   token_to_index_idf_dict = {token : math.log(self.num_of_docs / (self.df[token] + epsilon), 10) for token in unique_query_tokens}\n","  #   token_to_query_tf_dict = {token: query_counter[token] / num_of_tokens_in_query for token in unique_query_tokens}\n","  #   token_to_dict_of_candidates_docs = {token : self.read_posting_list_to_dict(token) for token in unique_query_tokens}\n","  #   union_of_candidates_docs = {doc_id for posting_list_dict in token_to_dict_of_candidates_docs.values() for doc_id in posting_list_dict.keys()}\n","  #   candidates_docs_rdd = sc.parallelize(union_of_candidates_docs)\n","  #   docs_result_rdd = candidates_docs_rdd.map(lambda candidate_doc_id: (candidate_doc_id, [(token_to_dict_of_candidates_docs[token][candidate_doc_id] / self.DL[candidate_doc_id]) * token_to_index_idf_dict[token] if candidate_doc_id in token_to_dict_of_candidates_docs[token] else 0 for token in unique_query_tokens]))\n","  #   query_result = [token_to_query_tf_dict[token] * token_to_index_idf_dict[token] for token in unique_query_tokens]\n","  #   print(docs_result_rdd.take(300))\n","  #   return docs_result_rdd, query_result\n","\n","  # def generate_tfidf_scores_with_read_rdd(self, query_to_search):\n","  #   sc = SparkContext.getOrCreate()\n","  #   epsilon = .0000001\n","  #   query_counter = Counter(query_to_search)\n","  #   num_of_tokens_in_query = len(query_to_search)\n","  #   unique_query_tokens = list(set(query_to_search))\n","  #   num_of_unique_tokens_in_query = len(unique_query_tokens)\n","  #   token_to_index_idf_dict = {token : math.log(self.num_of_docs / (self.df[token] + epsilon), 10) for token in unique_query_tokens}\n","  #   token_to_query_tf_dict = {token: query_counter[token] / num_of_tokens_in_query for token in unique_query_tokens}\n","\n","  #   token_to_rdd = {token : self.read_posting_list_to_rdd(token) for token in unique_query_tokens}\n","  #   token_to_scores_dict = defaultdict()\n","  #   for token in unique_query_tokens:\n","  #     token_to_rdd[token] = token_to_rdd[token].map(lambda doc_id_to_freq: (doc_id_to_freq[0], (doc_id_to_freq[1] / self.DL[doc_id_to_freq[0]]) * token_to_index_idf_dict[token]))\n","  #     token_to_scores_dict[token] = dict(token_to_rdd[token].top(K_TOP_ELEMENTS, key=lambda doc_id_to_tfidf_score: doc_id_to_tfidf_score[1]))\n","\n","  #   doc_id_to_scores_list_dict = defaultdict()\n","  #   for token_index, token in enumerate(unique_query_tokens):\n","  #     for doc_id, tfidf_score in token_to_scores_dict[token].items():\n","  #       if doc_id not in doc_id_to_scores_list_dict:\n","  #         doc_id_to_scores_list_dict[doc_id] = [0] * num_of_unique_tokens_in_query\n","  #       doc_id_to_scores_list_dict[doc_id][token_index] = tfidf_score\n","  #   query_result = [token_to_query_tf_dict[token] * token_to_index_idf_dict[token] for token in unique_query_tokens]\n","  #   return doc_id_to_scores_list_dict, query_result\n","  \n","  # def calculate_cossim_by_weights_of_tfidf_for_search_body(self, query_to_search):\n","  #   '''\n","  #     returns a list of the top 100 docs matching this query according to cosine similarity score.\n","  #   '''\n","  #   # result = list()\n","  #   doc_id_to_list_of_tfidf_scores_rdd, query_tfidf_scores_list = self.generate_tfidf_scores_rdd_and_vector(query_to_search)\n","  #   query_magnitude = math.sqrt(sum([x ** 2 for x in query_tfidf_scores_list]))\n","  #   dot_product_rdd = doc_id_to_list_of_tfidf_scores_rdd.map(lambda doc_id_to_scores_list: (doc_id_to_scores_list[0], sum([x * y for x, y in zip(doc_id_to_scores_list[1], query_tfidf_scores_list)])))\n","  #   cossim_score = dot_product_rdd.map(lambda doc_id_to_dot_product: (doc_id_to_dot_product[0], doc_id_to_dot_product[1] / (query_magnitude * self.doc_id_to_norm[doc_id_to_dot_product[0]])))\n","  #   # sorted_doc_id_to_cossim_score_rdd = cossim_score.sortBy(lambda doc_id_to_cossim_score: doc_id_to_cossim_score[1], ascending=False)\n","  #   # sorted_doc_id_to_cossim_score_rdd_with_index = sorted_doc_id_to_cossim_score_rdd.zipWithIndex()\n","  #   # sorted_doc_id_to_cossim_score_rdd_top_100 = sorted_doc_id_to_cossim_score_rdd_with_index.filter(lambda x: x[1] < 100).map(lambda x: x[0])\n","  #   sorted_doc_id_to_cossim_score_top_100 = cossim_score.top(100, key=lambda doc_id_to_cossime_score: doc_id_to_cossime_score[1])\n","  #   # result_rdd = sorted_doc_id_to_cossim_score_rdd_top_100.map(lambda doc_id_to_cossim_score: (doc_id_to_cossim_score[0], self.doc_id_to_title[doc_id_to_cossim_score[0]], doc_id_to_cossim_score[1]))\n","  #   # docs_tfidf_dict, query_tfidf_vector = self.generate_tfidf_scores_vectors(query_to_search)\n","  #   # query_magnitude = math.sqrt(sum([x ** 2 for x in query_tfidf_vector]))\n","  #   # for doc_id, scores_vector in docs_tfidf_dict.items():\n","  #   #   dot_product = sum([x * y for x, y in zip(scores_vector, query_tfidf_vector)])\n","  #   #   doc_magnitude = self.doc_id_to_norm[doc_id]\n","  #   #   cossim_score = dot_product / (query_magnitude * doc_magnitude)\n","  #   #   heapq.heappush(result, (-1 * cossim_score, doc_id))\n","  #   # sorted_top_100_docs = heapq.nsmallest(TOP_N_FOR_SEARCH_BODY, result)\n","  #   return [(doc_id, self.doc_id_to_title[doc_id]) for doc_id, score in sorted_doc_id_to_cossim_score_top_100]\n","\n","  # def calculate_cossim_by_weights_of_tfidf_old(self, query_to_search):\n","  #   '''\n","  #     returns an RDD of all the docs matching this query according to cosine similarity score.\n","  #   '''\n","  #   doc_id_to_list_of_tfidf_scores_rdd, query_tfidf_scores_list = self.generate_tfidf_scores_rdd_and_vector_with_read_rdd(query_to_search)\n","  #   query_magnitude = math.sqrt(sum([x ** 2 for x in query_tfidf_scores_list]))\n","  #   dot_product_rdd = doc_id_to_list_of_tfidf_scores_rdd.map(lambda doc_id_to_scores_list: (doc_id_to_scores_list[0], sum([x * y for x, y in zip(doc_id_to_scores_list[1], query_tfidf_scores_list) if x is not None])))\n","  #   cossim_score = dot_product_rdd.map(lambda doc_id_to_dot_product: (doc_id_to_dot_product[0], doc_id_to_dot_product[1] / (query_magnitude * self.doc_id_to_norm[doc_id_to_dot_product[0]])))\n","  #   # sorted_doc_id_to_cossim_score_top_100 = cossim_score.top(100, key=lambda doc_id_to_cossime_score: doc_id_to_cossime_score[1])\n","  #   return cossim_score\n","\n","  def calculate_cossim_by_weights_of_tfidf(self, query_to_search, num_of_results, is_all_results):\n","    result = list()\n","    doc_id_to_scores_list_dict, query_tfidf_scores_list = self.generate_tfidf_scores_vectors(query_to_search, is_all_results)\n","    query_magnitude = math.sqrt(sum([x ** 2 for x in query_tfidf_scores_list]))\n","    for doc_id, scores_list in doc_id_to_scores_list_dict.items():\n","      dot_product = sum([x * y for x, y in zip(scores_list, query_tfidf_scores_list)])\n","      doc_magnitude = self.doc_id_to_norm[doc_id]\n","      cossim_score = dot_product / (query_magnitude * doc_magnitude)\n","      heapq.heappush(result, (-1 * cossim_score, doc_id))\n","    \n","    sorted_top_num_of_docs = heapq.nsmallest(num_of_results, result)\n","    return [(doc_id, (score * -1)) for score, doc_id in sorted_top_num_of_docs]\n"],"metadata":{"id":"k6WndNHQBL31"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BM25"],"metadata":{"id":"drgw3aH_XlaQ"}},{"cell_type":"code","source":["class InvertedIndex(InvertedIndex): \n","  def generate_doc_id_to_BM25_score_dict_new(self, query_to_search, b, k1, k3, is_all_results):\n","    epsilon = 0.000001\n","    query_counter = Counter(query_to_search)\n","    num_of_tokens_in_query = len(query_to_search)\n","    unique_query_tokens = list(set(query_to_search))\n","    num_of_unique_tokens_in_query = len(unique_query_tokens)\n","    token_to_index_idf_dict = {token : math.log((self.num_of_docs + 1) / (self.df[token] + epsilon), 10) for token in unique_query_tokens}\n","    # token_to_query_tf_tag_dict = {token: ((k3 + 1) * (query_counter[token] / num_of_tokens_in_query)) / (k3 + (query_counter[token] / num_of_tokens_in_query)) for token in unique_query_tokens}\n","    token_to_posting_list_dict_dict = {token : self.read_posting_list_to_dict(token, is_all_results) for token in unique_query_tokens}\n","    doc_id_to_BM25_score_dict = defaultdict()\n","    for token in unique_query_tokens:\n","      for doc_id, freq in token_to_posting_list_dict_dict[token].items():\n","        B = 1 - b + (b * (self.DL[doc_id] / self.avg_dl))\n","        tf_doc = freq / self.DL[doc_id]\n","        doc_id_to_BM25_score_dict[doc_id] = doc_id_to_BM25_score_dict.get(doc_id, 0) + ((((k1 + 1) * tf_doc) / (B*k1 + tf_doc)) * token_to_index_idf_dict[token])\n","    return doc_id_to_BM25_score_dict\n","\n","  # def generate_doc_id_to_BM25_score_rdd(self, query_to_search, b, k1, k3):\n","  #   '''\n","  #     returns an RDD of all the docs matching this query according to BM25 score.\n","  #   '''\n","  #   sc = SparkContext.getOrCreate()\n","  #   epsilon = 0\n","  #   query_counter = Counter(query_to_search)\n","  #   num_of_tokens_in_query = len(query_to_search)\n","  #   unique_query_tokens = set(query_to_search)\n","  #   token_to_index_idf_dict = {token : math.log((self.num_of_docs + 1) / (self.df[token] + epsilon), 10) for token in unique_query_tokens}\n","  #   token_to_query_tf_tag_dict = {token: ((k3 + 1) * (query_counter[token] / num_of_tokens_in_query)) / (k3 + (query_counter[token] / num_of_tokens_in_query)) for token in unique_query_tokens}\n","  #   token_to_dict_of_candidates_docs = {token : self.read_posting_list_to_dict(token) for token in unique_query_tokens}\n","  #   union_of_candidates_docs = {doc_id for posting_list_dict in token_to_dict_of_candidates_docs.values() for doc_id in posting_list_dict.keys()}\n","  #   candidates_docs_rdd = sc.parallelize(union_of_candidates_docs)\n","  #   doc_id_to_BM25_scores_rdd = candidates_docs_rdd.map(lambda candidate_doc_id: (candidate_doc_id, [(((k1 + 1) * (token_to_dict_of_candidates_docs[token][candidate_doc_id] / self.DL[candidate_doc_id])) / (((1 - b + (b * (self.DL[candidate_doc_id] / AVG_DL))) * k1) + (token_to_dict_of_candidates_docs[token][candidate_doc_id] / self.DL[candidate_doc_id]))) * token_to_index_idf_dict[token] * token_to_query_tf_tag_dict[token] if candidate_doc_id in token_to_dict_of_candidates_docs[token] else 0 for token in unique_query_tokens]))\n","  #   BM25_scores_rdd = doc_id_to_BM25_scores_rdd.map(lambda doc_id_to_list: (doc_id_to_list[0], sum(doc_id_to_list[1])))\n","  #   # sorted_doc_id_to_BM25_score_top_100 = BM25_scores_rdd.top(100, key=lambda doc_id_to_BM25_score: doc_id_to_BM25_score[1])\n","  #   return BM25_scores_rdd\n","\n","  # def generate_doc_id_to_BM25_score_dict(self, query_to_search, b, k1, k3):\n","  #   '''\n","  #     returns an RDD of all the docs matching this query according to BM25 score.\n","  #   '''\n","  #   sc = SparkContext.getOrCreate()\n","  #   epsilon = 0.000001\n","  #   query_counter = Counter(query_to_search)\n","  #   num_of_tokens_in_query = len(query_to_search)\n","  #   unique_query_tokens = list(set(query_to_search))\n","  #   num_of_unique_tokens_in_query = len(unique_query_tokens)\n","  #   token_to_index_idf_dict = {token : math.log((self.num_of_docs + 1) / (self.df[token] + epsilon), 10) for token in unique_query_tokens}\n","  #   token_to_query_tf_tag_dict = {token: ((k3 + 1) * (query_counter[token] / num_of_tokens_in_query)) / (k3 + (query_counter[token] / num_of_tokens_in_query)) for token in unique_query_tokens}\n","  #   token_to_rdd = {token : self.read_posting_list_to_rdd(token) for token in unique_query_tokens}\n","  #   token_to_scores_dict = defaultdict()\n","  #   for token in unique_query_tokens:\n","  #     token_to_rdd[token] = token_to_rdd[token].map(lambda doc_id_to_freq: (doc_id_to_freq[0], (((k1 + 1) * (doc_id_to_freq[1] / self.DL[doc_id_to_freq[0]])) / (((1 - b + (b * (self.DL[doc_id_to_freq[0]] / AVG_DL))) * k1) + (doc_id_to_freq[1] / self.DL[doc_id_to_freq[0]]))) * token_to_index_idf_dict[token] * token_to_query_tf_tag_dict[token]))\n","  #     token_to_scores_dict[token] = dict(token_to_rdd[token].top(K_TOP_ELEMENTS, key=lambda doc_id_to_BM25_score: doc_id_to_BM25_score[1]))\n","      \n","  #   doc_id_to_BM25_score_dict = defaultdict()\n","  #   for token in unique_query_tokens:\n","  #     for doc_id, BM25_score in token_to_scores_dict[token].items():\n","  #       current_BM25_score = doc_id_to_BM25_score_dict.setdefault(doc_id, 0)\n","  #       doc_id_to_BM25_score_dict[doc_id] = current_BM25_score + BM25_score\n","\n","  #   return doc_id_to_BM25_score_dict\n","\n","  def calculate_BM25_scores(self, query_to_search, b, k1, k3, num_of_results, is_all_results):\n","    result = list()\n","    doc_id_to_BM25_scores_dict = self.generate_doc_id_to_BM25_score_dict_new(query_to_search, b, k1, k3, is_all_results)\n","    for doc_id, BM25_score in doc_id_to_BM25_scores_dict.items():\n","      heapq.heappush(result, (-1 * BM25_score, doc_id))\n","    \n","    sorted_top_num_of_docs = heapq.nsmallest(num_of_results, result)\n","    return [(doc_id, (score * -1)) for score, doc_id in sorted_top_num_of_docs]\n"],"metadata":{"id":"5yeWDeiHXpEV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Boolean function methods in InvertedIndex class"],"metadata":{"id":"jTXzoYPxCeSk"}},{"cell_type":"code","source":["class InvertedIndex (InvertedIndex):\n","  def rank_by_boolean_scores(self, query_to_search, is_all_results):\n","      \"\"\" \n","      Parameters:\n","      -----------\n","      query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                      Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","      index:           inverted index loaded from the corresponding files.    \n","      \n","      Returns:\n","      -----------\n","      docs_result: {key: doc_id, value: vector (list) of query words docs scores}\n","      \"\"\"\n","      docs_to_boolean_score = defaultdict()\n","      unique_query_tokens = set(query_to_search)\n","      for token in unique_query_tokens:\n","        if token in self.df.keys(): #avoid terms that do not appear in the index.  \n","          list_of_candidates_docs = self.read_posting_list(token, is_all_results)\n","          for doc_id, freq in list_of_candidates_docs:\n","            docs_to_boolean_score[doc_id] = docs_to_boolean_score.get(doc_id, 0) + 1\n","      sorted_docs_to_boolean_score = sorted(docs_to_boolean_score.items(), key = lambda doc_id_to_score: doc_id_to_score[1], reverse = True)\n","      # return [(doc_id, self.doc_id_to_title[doc_id]) for doc_id, score in sorted_docs_to_boolean_score]\n","      return sorted_docs_to_boolean_score"],"metadata":{"id":"pQ_U9lJqCRTA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Searching methods"],"metadata":{"id":"Bz9gGKptULwO"}},{"cell_type":"code","source":["class InvertedIndex(InvertedIndex):\n","  def search_body(self, query_to_search_string):\n","    processed_query_tokens_list = process_text(0, query_to_search_string)[1]\n","    # matching_docs_to_score_rdd = self.calculate_cossim_by_weights_of_tfidf(processed_query_tokens_list)\n","    # sorted_doc_id_to_cossim_score_top_100 = matching_docs_to_score_rdd.top(100, key=lambda doc_id_to_cossime_score: doc_id_to_cossime_score[1])\n","    sorted_cossim_score_top_100 = self.calculate_cossim_by_weights_of_tfidf(processed_query_tokens_list, 100, is_all_results=False)\n","    return [(doc_id, self.doc_id_to_title[doc_id]) for doc_id, score in sorted_cossim_score_top_100]\n","\n","  def search_anchor(self, query):\n","    processed_query_tokens_list = process_text(0, query)[1]\n","    return self.rank_by_boolean_scores(processed_query_tokens_list, is_all_results=True)\n","\n","  def search_title(self, query):\n","    processed_query_tokens_list = process_text(0, query)[1]\n","    return self.rank_by_boolean_scores(processed_query_tokens_list, is_all_results=True)\n","\n","  def search_cossim(self, query_to_search_string, num_of_results):\n","    processed_query_tokens_list = process_text(0, query_to_search_string)[1]\n","    sorted_doc_id_to_cossim_score_top_n = self.calculate_cossim_by_weights_of_tfidf(processed_query_tokens_list, num_of_results, is_all_results=False)\n","    return [(doc_id, self.doc_id_to_title[doc_id], score) for doc_id, score in sorted_doc_id_to_cossim_score_top_n]\n","\n","  def search_BM25(self, query_to_search_string, b, k1, k3, num_of_results):\n","    processed_query_tokens_list = process_text(0, query_to_search_string)[1]\n","    sorted_BM25_scores = self.calculate_BM25_scores(processed_query_tokens_list, b, k1, k3, num_of_results, is_all_results=False)\n","    # matching_docs_to_score_rdd = self.generate_doc_id_to_BM25_score_rdd(processed_query_tokens_list, b, k1, k3)\n","    # sorted_doc_id_to_cossim_score_top_100 = matching_docs_to_score_rdd.top(100, key=lambda doc_id_to_cossime_score: doc_id_to_cossime_score[1])\n","    return [(doc_id, self.doc_id_to_title[doc_id], score) for doc_id, score in sorted_BM25_scores]\n"],"metadata":{"id":"vU57PODLURrk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Turning our csv to DataFrame**"],"metadata":{"id":"MCNoj5ZyA3ke"}},{"cell_type":"code","source":["def import_dataframe_from_csv(csv_path):\n","  '''\n","  parameter: csv path of dataframe object (for example \"/home/dataproc/test.csv\")\n","  return: spark's dataframe object of (doc_id, title, body, anchor_text)\n","  '''\n","  pandas_copy_of_relevant_docs = pd.read_csv(csv_path)\n","  return spark.createDataFrame(pandas_copy_of_relevant_docs)"],"metadata":{"id":"xnJtggt2BBCZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Global variables and Constants**"],"metadata":{"id":"STtyI4OCqIoo"}},{"cell_type":"code","source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","MINIMUM_DF = 10\n","\n","TOP_N_FOR_SEARCH_BODY = 100\n","\n","PAGE_RANKS_PATH = '/content/doc_id_to_pagerank_dict.pkl'\n","\n","PAGE_VIEWS_PATH = '/content/doc_id_to_pageview_dict.pkl'\n","\n","docs_data_frame = import_dataframe_from_csv('/content/nirs_relevant_documents_from_gcp.csv')\n","\n","b = 0.75\n","\n","AVG_DL = 1758.6576955424728\n","# average([dl for dl in relevant_body_inverted_index.DL.values()])\n","\n","K_TOP_ELEMENTS = 400"],"metadata":{"id":"YXasWr_-qQUo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Text Processing**"],"metadata":{"id":"1yVgzbMfqhNj"}},{"cell_type":"code","source":["def extract_id_title(dataframe):\n","  return dataframe.select(\"id\", \"title\").rdd\n","\n","def extract_id_body(dataframe):\n","  return dataframe.select(\"id\", \"text\", \"title\").rdd\n","\n","def extract_id_anchor_text(dataframe):\n","  anchors_rdd = dataframe.select(\"anchor_text\").rdd\n","  doc_id_as_string_to_text_as_string = anchors_rdd.map(lambda anchor_id_string_to_text_string_as_one_string: re.findall(r\"\\bid=(\\d+), text='([^']+)'\", anchor_id_string_to_text_string_as_one_string[0])).flatMap(lambda anchors_row: anchors_row)\n","  doc_id_as_number_to_text_as_string = doc_id_as_string_to_text_as_string.map(lambda anchor_id_to_text: (int(anchor_id_to_text[0]), anchor_id_to_text[1]))\n","  doc_id_as_number_to_text_as_string_no_duplicated_texts = doc_id_as_number_to_text_as_string.distinct()\n","  doc_id_as_number_to_word_as_string_no_duplicated_words = doc_id_as_number_to_text_as_string_no_duplicated_texts.flatMap(lambda doc_id_to_text: [(doc_id_to_text[0], word_in_text.group()) for word_in_text in RE_WORD.finditer(doc_id_to_text[1].lower())]).distinct()\n","  return doc_id_as_number_to_word_as_string_no_duplicated_words\n","  # return anchors_rdd.flatMap(lambda anchors_row: anchors_row) # real working return for the furture\n","\n","def extract_rdd_from_data_frame(dataframe, title=False, body=False, anchor=False):\n","  if title:\n","    return extract_id_title(dataframe)\n","  elif body:\n","    return extract_id_body(dataframe)\n","  else:\n","    return extract_id_anchor_text(dataframe)"],"metadata":{"id":"S5Iht5Y4Ft8w"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTSDrwk8icSU"},"outputs":[],"source":["stemmer = PorterStemmer()\n","def process_text(id, text, use_stemming=False):\n","    \"\"\"\n","    This function aims in:\n","    1. tokenize\n","    2. filter stopwords\n","    3. do stemming\n","    \n","    Parameters:\n","    -----------\n","    id: document id\n","    text: string , represting the text to tokenize.    \n","    \n","    Returns:\n","    -----------\n","    tuple of (id, list of tokens (e.g., list of tokens)).\n","    \"\"\"\n","    if use_stemming:\n","      list_of_tokens = list()\n","      for token in RE_WORD.finditer(text.lower()):\n","        stemmed_token = stemmer.stem(token.group())\n","        if stemmed_token not in all_stopwords:\n","          list_of_tokens.append(stemmed_token)\n","    else:\n","      list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n","    return id, list_of_tokens"]},{"cell_type":"markdown","source":["## **Functions for creating the full index**"],"metadata":{"id":"5XIE5AYNVxro"}},{"cell_type":"code","source":["NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS"],"metadata":{"id":"iNXaReLZV5yl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def word_count(id, list_of_tokens):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  # YOUR CODE HERE\n","  return [(item[0], (id, item[1])) for item in Counter(list_of_tokens).items()]"],"metadata":{"id":"9yTc2dwkXQ69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InvertedIndex(InvertedIndex):\n","  def reduce_word_counts(self, token_to_unsorted_pl_tuple):\n","    ''' Returns a sorted posting list by wiki_id.\n","    Parameters:\n","    -----------\n","      unsorted_pl: list of tuples\n","        A list of (wiki_id, tf) tuples \n","    Returns:\n","    --------\n","      list of tuples\n","        A sorted posting list.\n","    '''\n","    # YOUR CODE HERE\n","    idf = math.log((self.num_of_docs / self.df[token_to_unsorted_pl_tuple[0]]), 10)\n","    pl = token_to_unsorted_pl_tuple[1]\n","    unsorted_pl = [(doc_id, freq, (freq / self.DL[doc_id]) * idf) for doc_id, freq in pl]\n","    sorted_pl = sorted(unsorted_pl, key= lambda x: x[2])\n","    return (token_to_unsorted_pl_tuple[0], [(pl[0], pl[1]) for pl in sorted_pl])"],"metadata":{"id":"Dn-D0s9KXN1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.mapValues(lambda p: len(p))"],"metadata":{"id":"md7OnyUdXL2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def partition_postings_and_write(postings, base_dir):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  buckets_posting_lists = postings.map(lambda p: (token2bucket_id(p[0]), p))\n","  buckets_posting_lists = buckets_posting_lists.groupByKey()\n","  postings_locs = buckets_posting_lists.map(lambda bpl: InvertedIndex.write_a_posting_list(bpl, base_dir))\n","  return postings_locs"],"metadata":{"id":"6pu0B5D0XJnn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Putting it all together**"],"metadata":{"id":"73iGsk_7VKn5"}},{"cell_type":"code","source":["def calculate_norm_of_tokens_tfidf_in_doc(tokens_list, num_of_docs, df):\n","  num_of_tokens = len(tokens_list)\n","  counter = Counter(tokens_list)\n","  uniqe_tokens_set = set(tokens_list)\n","  sum_of_squared_scores = 0\n","  for token in uniqe_tokens_set:\n","    if token in df:\n","      sum_of_squared_scores += (counter[token] / num_of_tokens * math.log(num_of_docs / df[token], 10)) ** 2\n","  return math.sqrt(sum_of_squared_scores)"],"metadata":{"id":"T7pRV7iq7zgq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_inverted_index(dataframe, inverted_index_name, title=False, body=False, anchor=False, use_stemming=False, filter_by_minimum_df=False):\n","  id_to_content_rdd = extract_rdd_from_data_frame(dataframe, title, body, anchor)\n","  processed_id_to_content_rdd = id_to_content_rdd.map(lambda id_content_tuple: process_text(id_content_tuple[0], id_content_tuple[1], use_stemming))\n","  \n","  # Create inverted index instance\n","  inverted = InvertedIndex()\n","\n","  DL_rdd = processed_id_to_content_rdd.map(lambda id_content_tuple: (id_content_tuple[0], len(id_content_tuple[1])))\n","  inverted.DL = dict(DL_rdd.collect())\n","  inverted.avg_dl = DL_rdd.map(lambda doc_id_to_dl: doc_id_to_dl[1]).mean()\n","\n","  # word counts map\n","  word_counts = processed_id_to_content_rdd.flatMap(lambda x: word_count(x[0], x[1]))\n","  postings = word_counts.groupByKey()\n","\n","  # filtering postings and calculate df\n","  if filter_by_minimum_df:\n","    postings = postings.filter(lambda x: len(x[1]) > MINIMUM_DF)\n","  \n","  # calculating df\n","  w2df = calculate_df(postings)\n","  w2df_dict = w2df.collectAsMap()\n","\n","  # Add the token - df dictionary to the inverted index\n","  inverted.df = w2df_dict\n","  inverted.num_of_docs = processed_id_to_content_rdd.count()\n","\n","  # sorting posting lists by tf-idf score\n","  postings = postings.map(lambda token_to_pl_tuple: inverted.reduce_word_counts(token_to_pl_tuple))\n","\n","  # partition posting lists and write out\n","  posting_locs_list = partition_postings_and_write(postings, './' + inverted_index_name).collect()\n","\n","\n","  # merge the posting locations into a single dict\n","  super_posting_locs = defaultdict(list)\n","  for posting_loc in posting_locs_list:\n","    for k, v in posting_loc.items():\n","      super_posting_locs[k].extend(v)\n","\n","\n","  # Adding the posting locations dictionary to the inverted index\n","  inverted.posting_locs = super_posting_locs\n","\n","  # creating dictionary that maps doc id to norm, to cosine calculation\n","  doc_id_to_norm_rdd = processed_id_to_content_rdd.map(lambda id_content_tuple: (id_content_tuple[0], calculate_norm_of_tokens_tfidf_in_doc(id_content_tuple[1], inverted.num_of_docs, inverted.df)))\n","  inverted.doc_id_to_norm = dict(doc_id_to_norm_rdd.collect())\n","  \n","  if title:\n","    id_to_title_rdd = id_to_content_rdd.map(lambda doc_id_to_content_and_title: (doc_id_to_content_and_title[0], doc_id_to_content_and_title[1]))\n","    inverted.doc_id_to_title = dict(id_to_title_rdd.collect())\n","  elif body:\n","    id_to_title_rdd = id_to_content_rdd.map(lambda doc_id_to_content_and_title: (doc_id_to_content_and_title[0], doc_id_to_content_and_title[2]))\n","    inverted.doc_id_to_title = dict(id_to_title_rdd.collect())\n","  \n","  # write the global stats out\n","  inverted.write_index('.', inverted_index_name)\n","\n","  return inverted"],"metadata":{"id":"uvY-1jHdYvru"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Testing**"],"metadata":{"id":"KArSz8x7nT05"}},{"cell_type":"markdown","source":["## **Creating indexes**"],"metadata":{"id":"r0jGMxdCnkvH"}},{"cell_type":"code","source":["relevant_title_inverted_index = create_inverted_index(docs_data_frame, 'relevant_title_index', title=True)"],"metadata":{"id":"jpcNfie-kD57"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["relevant_body_inverted_index = create_inverted_index(docs_data_frame, 'relevant_body_index', body=True)"],"metadata":{"id":"gJPRB0LadjAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["relevant_anchor_inverted_index = create_inverted_index(docs_data_frame, 'relevant_anchor_index', anchor=True)"],"metadata":{"id":"mDKP8o57rRNn","executionInfo":{"status":"error","timestamp":1673599960924,"user_tz":-120,"elapsed":29399,"user":{"displayName":"Amit Kravchik","userId":"18309304432468869556"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3cbb7d47-6f22-4d9f-ade4-8f925473f9aa"},"execution_count":null,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-0a5d48d0ed55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelevant_anchor_inverted_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_inverted_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_data_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relevant_anchor_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-1372197f31e3>\u001b[0m in \u001b[0;36mcreate_inverted_index\u001b[0;34m(dataframe, inverted_index_name, title, body, anchor, use_stemming, filter_by_minimum_df)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# partition posting lists and write out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mposting_locs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition_postings_and_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minverted_index_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 16.0 failed 1 times, most recent failure: Lost task 1.0 in stage 16.0 (TID 15) (28496c93a98a executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\", line 2665, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 253, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-21-1372197f31e3>\", line 29, in <lambda>\n  File \"<ipython-input-17-42f9d7a0d3f5>\", line 16, in reduce_word_counts\n  File \"<ipython-input-17-42f9d7a0d3f5>\", line 16, in <listcomp>\nZeroDivisionError: division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\", line 2665, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 253, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/lib/python3.8/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-21-1372197f31e3>\", line 29, in <lambda>\n  File \"<ipython-input-17-42f9d7a0d3f5>\", line 16, in reduce_word_counts\n  File \"<ipython-input-17-42f9d7a0d3f5>\", line 16, in <listcomp>\nZeroDivisionError: division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]}]},{"cell_type":"markdown","source":["## **Testing search_body**"],"metadata":{"id":"o5RgbXx_duje"}},{"cell_type":"code","source":["search_body_query_string = 'air jordan'\n","search_body_result = relevant_body_inverted_index.search_cossim(search_body_query_string, 100)\n","print(search_body_result)"],"metadata":{"id":"6SaD15RFwq2x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673447267045,"user_tz":-120,"elapsed":524,"user":{"displayName":"Amit Kravchik","userId":"18309304432468869556"}},"outputId":"c3f99aea-438e-4b70-b937-8d21ae942ed7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(1394509, 'Air Jordan', 0.8169141417766355), (1371219, 'Michael Jordan (disambiguation)', 0.617379499216125), (20455, 'Michael Jordan', 0.5667623326130194), (58209447, 'Air Jordan (airline)', 0.5573413013217093), (23353937, 'Michael Jordan: An American Hero', 0.5404345069176116), (28155315, 'Air Arabia Jordan', 0.4818663985661909), (3097723, 'James R. Jordan Sr.', 0.4377720569970151), (13365219, \"Jordan Spiz'ike\", 0.3886234691017801), (4253801, 'Jumpman (logo)', 0.3880206065496625), (50066979, 'Crying Jordan', 0.3598641218977917), (2310146, 'Michael B. Jordan', 0.32305011258296407), (7851893, 'Amman Civil Airport', 0.27486618335989477), (265033, 'Space Jam', 0.2582825737501759), (51546226, 'Mike Jordan (cornerback)', 0.2107362607973597), (62741501, 'The Last Dance (miniseries)', 0.19903474357353976), (20657443, 'Jordanian cuisine', 0.18091273147952283), (26457880, 'Air India', 0.17860361592994173), (3647739, 'Jordans', 0.17483248361646767), (3890370, 'Michael-Hakim Jordan', 0.14679765579273676), (9998569, 'Sneaker collecting', 0.12816962008478006), (6722408, 'List of career achievements by Michael Jordan', 0.12017750638763122), (60601430, 'Michael Jordan (offensive lineman)', 0.11203740700569058), (67838974, 'Jordan Geller', 0.07654529710645987), (105344, 'Royal Jordanian', 0.05834550172013357), (15183570, 'MacBook Air', 0.057530775284981524), (11577897, 'Makdous', 0.05740187574025835), (18998781, 'Nike, Inc.', 0.04961061917010075), (30687447, 'I Used to Be Fat', 0.04125351126571729), (15295713, 'MacBook', 0.03986018227609822), (52780757, 'List of psychoactive drugs used by militaries', 0.039474947042488495), (164311, 'Falafel', 0.03485793225881302), (41677925, 'Black Panther (film)', 0.031246581890094778), (3757402, 'Microfoam', 0.029677928564185066), (4925720, 'Khubz', 0.02811357379257124), (63534797, 'Beaten coffee', 0.027903084183789246), (1039663, 'Tabbouleh', 0.027690602584804616), (2243880, 'Ful medames', 0.025566563769256038), (57374888, 'Basics of white flower colouration', 0.024642829131429407), (53837, 'Salt Lake City', 0.022036034323415627), (7329519, 'West Bank Story', 0.021883948217575062), (60534017, 'The Simpsons (season 33)', 0.02159719375794887), (4478297, 'Timeline of Macintosh models', 0.020578170144241126), (16615604, 'Winter Palace', 0.02036235911309526), (65464184, 'Carol Danvers (Marvel Cinematic Universe)', 0.020104311171185708), (47863605, 'Middle Eastern cuisine', 0.01968889779961715), (2828101, 'Three Tales (opera)', 0.01775782016635298), (884998, 'Fox Kids', 0.01754852156997015), (17997437, 'Timeline of Apple Inc. products', 0.017196734386381658), (1031040, 'Fermentation lock', 0.017165105970644474), (9833167, 'Fast Forward (film)', 0.016077933645527436), (36916362, 'Jordan Nkololo', 0.016046117918662034), (19006979, 'Macintosh', 0.015418563927096245), (2165666, 'White coffee', 0.01534022165836717), (43603241, 'Captain Marvel (film)', 0.015200219648413447), (4512923, 'Cannabis smoking', 0.014839361869897504), (17349106, 'Ariel Winter', 0.01454033928216635), (82789, 'Pita', 0.014474243758609607), (36971117, 'Hyperloop', 0.014173668876376541), (43343961, 'Arab Winter', 0.013712905718212527), (1472206, 'Economy of India', 0.01357963292774401), (8438818, 'Winter of 1962–63 in the United Kingdom', 0.013422105851903359), (8351234, '2022 Winter Olympics', 0.013083776208178886), (1632099, 'Winter road', 0.013058375725495513), (19159283, 'Seoul', 0.01304303864246196), (3915251, 'Winston (cigarette)', 0.012901521058506263), (5964683, 'Moka pot', 0.01247442116640933), (37756, 'Delhi', 0.012449810606065767), (3038969, 'The Simpsons house', 0.012396109152972313), (49134382, 'Raising Gazorpazorp', 0.01224110276341499), (49051658, 'My Diet Is Better Than Yours', 0.011777141622121323), (65819511, 'Rick and Morty (season 5)', 0.011601130513162248), (51430647, 'Music of the Marvel Cinematic Universe', 0.01096075243841361), (1372169, 'Alex Winter', 0.010691078102158974), (3099917, 'Levantine cuisine', 0.010586615554914433), (1649321, 'List of United States cities by population', 0.010183268470900675), (24452, 'Prime Minister of India', 0.009902156427110273), (5212064, 'Vacuum coffee maker', 0.009875574005215184), (1466966, 'The Simpsons shorts', 0.009860212689431627), (22777652, 'Ullage (wine)', 0.009795245506850722), (619983, 'List of Apple II games', 0.009685214346869073), (75065, 'Hummus', 0.00916757326326442), (18640, 'List of Macintosh models grouped by CPU type', 0.008922000455614901), (8046414, 'Justine Musk', 0.008858358675886637), (37249793, 'Used coffee grounds', 0.008795012156152101), (4506407, 'Arabic coffee', 0.008727526579293758), (42680256, 'Pollination bags', 0.008628385972057076), (678583, 'South India', 0.008248764600798389), (49387265, 'List of The Simpsons episodes (seasons 1–20)', 0.008002321362677763), (7555986, 'Fast Money (talk show)', 0.007975013523053319), (32670973, 'House of Cards (American TV series)', 0.007916466249091316), (1151047, 'The Biggest Loser (American TV series)', 0.007771705235537055), (68188835, 'Cannabis and sports', 0.007750095897763952), (5676692, 'Iron Man (2008 film)', 0.007733991486663006), (27318, 'Singapore', 0.007341795541706763), (9306179, 'History of The Simpsons', 0.0072615518494250275), (4776930, 'The Simpsons (season 18)', 0.0071804418214759394), (17596651, 'Ventilated cigarette', 0.007155107260069596), (1186115, 'Islam in India', 0.007090343735563603), (11447140, 'Israeli cuisine', 0.006980975237784368), (645042, 'New York City', 0.00691482450882912)]\n"]}]},{"cell_type":"code","source":["search_body_query_string = 'air jordan'\n","search_body_result = relevant_body_inverted_index.search_cossim(search_body_query_string, 100)\n","print(search_body_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O_smNnZCYrKV","executionInfo":{"status":"ok","timestamp":1673536472271,"user_tz":-120,"elapsed":418,"user":{"displayName":"Amit Kravchik","userId":"18309304432468869556"}},"outputId":"251b11a5-ee74-4739-97c6-fc35c6129389"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(1394509, 'Air Jordan', 0.8169141417766355), (1371219, 'Michael Jordan (disambiguation)', 0.617379499216125), (20455, 'Michael Jordan', 0.5667623326130194), (58209447, 'Air Jordan (airline)', 0.5573413013217093), (23353937, 'Michael Jordan: An American Hero', 0.5404345069176116), (28155315, 'Air Arabia Jordan', 0.4818663985661909), (3097723, 'James R. Jordan Sr.', 0.4377720569970151), (13365219, \"Jordan Spiz'ike\", 0.3886234691017801), (4253801, 'Jumpman (logo)', 0.3880206065496625), (50066979, 'Crying Jordan', 0.3598641218977917), (2310146, 'Michael B. Jordan', 0.32305011258296407), (7851893, 'Amman Civil Airport', 0.27486618335989477), (265033, 'Space Jam', 0.2582825737501759), (51546226, 'Mike Jordan (cornerback)', 0.2107362607973597), (62741501, 'The Last Dance (miniseries)', 0.19903474357353976), (20657443, 'Jordanian cuisine', 0.18091273147952283), (26457880, 'Air India', 0.17860361592994173), (3647739, 'Jordans', 0.17483248361646767), (3890370, 'Michael-Hakim Jordan', 0.14679765579273676), (9998569, 'Sneaker collecting', 0.12816962008478006), (6722408, 'List of career achievements by Michael Jordan', 0.12017750638763122), (60601430, 'Michael Jordan (offensive lineman)', 0.11203740700569058), (67838974, 'Jordan Geller', 0.07654529710645987), (105344, 'Royal Jordanian', 0.05834550172013357), (15183570, 'MacBook Air', 0.057530775284981524), (11577897, 'Makdous', 0.05740187574025835), (18998781, 'Nike, Inc.', 0.04961061917010075), (30687447, 'I Used to Be Fat', 0.04125351126571729), (15295713, 'MacBook', 0.03986018227609822), (52780757, 'List of psychoactive drugs used by militaries', 0.039474947042488495), (164311, 'Falafel', 0.03485793225881302), (41677925, 'Black Panther (film)', 0.031246581890094778), (3757402, 'Microfoam', 0.029677928564185066), (4925720, 'Khubz', 0.02811357379257124), (63534797, 'Beaten coffee', 0.027903084183789246), (1039663, 'Tabbouleh', 0.027690602584804616), (2243880, 'Ful medames', 0.025566563769256038), (57374888, 'Basics of white flower colouration', 0.024642829131429407), (53837, 'Salt Lake City', 0.022036034323415627), (7329519, 'West Bank Story', 0.021883948217575062), (60534017, 'The Simpsons (season 33)', 0.02159719375794887), (4478297, 'Timeline of Macintosh models', 0.020578170144241126), (16615604, 'Winter Palace', 0.02036235911309526), (65464184, 'Carol Danvers (Marvel Cinematic Universe)', 0.020104311171185708), (47863605, 'Middle Eastern cuisine', 0.01968889779961715), (2828101, 'Three Tales (opera)', 0.01775782016635298), (884998, 'Fox Kids', 0.01754852156997015), (17997437, 'Timeline of Apple Inc. products', 0.017196734386381658), (1031040, 'Fermentation lock', 0.017165105970644474), (9833167, 'Fast Forward (film)', 0.016077933645527436), (36916362, 'Jordan Nkololo', 0.016046117918662034), (19006979, 'Macintosh', 0.015418563927096245), (2165666, 'White coffee', 0.01534022165836717), (43603241, 'Captain Marvel (film)', 0.015200219648413447), (4512923, 'Cannabis smoking', 0.014839361869897504), (17349106, 'Ariel Winter', 0.01454033928216635), (82789, 'Pita', 0.014474243758609607), (36971117, 'Hyperloop', 0.014173668876376541), (43343961, 'Arab Winter', 0.013712905718212527), (1472206, 'Economy of India', 0.01357963292774401), (8438818, 'Winter of 1962–63 in the United Kingdom', 0.013422105851903359), (8351234, '2022 Winter Olympics', 0.013083776208178886), (1632099, 'Winter road', 0.013058375725495513), (19159283, 'Seoul', 0.01304303864246196), (3915251, 'Winston (cigarette)', 0.012901521058506263), (5964683, 'Moka pot', 0.01247442116640933), (37756, 'Delhi', 0.012449810606065767), (3038969, 'The Simpsons house', 0.012396109152972313), (49134382, 'Raising Gazorpazorp', 0.01224110276341499), (49051658, 'My Diet Is Better Than Yours', 0.011777141622121323), (65819511, 'Rick and Morty (season 5)', 0.011601130513162248), (51430647, 'Music of the Marvel Cinematic Universe', 0.01096075243841361), (1372169, 'Alex Winter', 0.010691078102158974), (3099917, 'Levantine cuisine', 0.010586615554914433), (1649321, 'List of United States cities by population', 0.010183268470900675), (24452, 'Prime Minister of India', 0.009902156427110273), (5212064, 'Vacuum coffee maker', 0.009875574005215184), (1466966, 'The Simpsons shorts', 0.009860212689431627), (22777652, 'Ullage (wine)', 0.009795245506850722), (619983, 'List of Apple II games', 0.009685214346869073), (75065, 'Hummus', 0.00916757326326442), (18640, 'List of Macintosh models grouped by CPU type', 0.008922000455614901), (8046414, 'Justine Musk', 0.008858358675886637), (37249793, 'Used coffee grounds', 0.008795012156152101), (4506407, 'Arabic coffee', 0.008727526579293758), (42680256, 'Pollination bags', 0.008628385972057076), (678583, 'South India', 0.008248764600798389), (49387265, 'List of The Simpsons episodes (seasons 1–20)', 0.008002321362677763), (7555986, 'Fast Money (talk show)', 0.007975013523053319), (32670973, 'House of Cards (American TV series)', 0.007916466249091316), (1151047, 'The Biggest Loser (American TV series)', 0.007771705235537055), (68188835, 'Cannabis and sports', 0.007750095897763952), (5676692, 'Iron Man (2008 film)', 0.007733991486663006), (27318, 'Singapore', 0.007341795541706763), (9306179, 'History of The Simpsons', 0.0072615518494250275), (4776930, 'The Simpsons (season 18)', 0.0071804418214759394), (17596651, 'Ventilated cigarette', 0.007155107260069596), (1186115, 'Islam in India', 0.007090343735563603), (11447140, 'Israeli cuisine', 0.006980975237784368), (645042, 'New York City', 0.00691482450882912)]\n"]}]},{"cell_type":"markdown","source":["## **Testing search_title**"],"metadata":{"id":"fvZDJ7HQkFMf"}},{"cell_type":"code","source":["search_title_query = 'fifa world cup'\n","search_title_result = relevant_title_inverted_index.search_title(search_title_query)\n","print(search_title_result)"],"metadata":{"id":"IHt2nhwVkaAR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673283348839,"user_tz":-120,"elapsed":268,"user":{"displayName":"Yuval Schwartz","userId":"10320755735300727416"}},"outputId":"a9a56233-0c92-4903-d581-3beb562a8d3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(11370, 3), (16383, 3), (59707, 3), (168079, 3), (244862, 3), (656933, 3), (1166428, 3), (1248592, 3), (1853149, 3), (2150801, 3), (2996777, 3), (3482503, 3), (3556431, 3), (4723188, 3), (4743361, 3), (8734046, 3), (8821389, 3), (10822574, 3), (13327177, 3), (16966712, 3), (17742072, 3), (19537336, 3), (26814387, 3), (27007503, 3), (27226732, 3), (29868391, 3), (32352129, 3), (32516422, 3), (36581929, 3), (39302261, 3), (41648358, 3), (42931572, 3), (45271353, 3), (55490096, 3), (57918704, 3), (57918711, 3), (60410401, 3), (60637832, 3), (61269058, 3), (61715824, 3), (62528055, 3), (64467696, 3), (64999764, 3), (64999924, 3), (66040084, 3), (66040086, 3), (67608822, 3), (33727, 2), (183628, 2), (8258172, 2), (16842834, 2), (22230053, 2), (39812824, 2), (51765484, 2), (57240806, 2), (22888933, 1), (29384326, 1), (36827305, 1), (42072639, 1), (63946361, 1)]\n"]}]},{"cell_type":"code","source":["search_title_query = 'fifa world cup'\n","search_title_result = relevant_title_inverted_index.search_title(search_title_query)\n","print(search_title_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PtqX4yPucF_n","executionInfo":{"status":"ok","timestamp":1673536488874,"user_tz":-120,"elapsed":266,"user":{"displayName":"Amit Kravchik","userId":"18309304432468869556"}},"outputId":"d60569fb-ea54-47eb-9b8b-bc9b70fd8f53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(39302261, 3), (64999924, 3), (60410401, 3), (60637832, 3), (61715824, 3), (62528055, 3), (66040084, 3), (66040086, 3), (64999764, 3), (27226732, 3), (32516422, 3), (57918704, 3), (57918711, 3), (67608822, 3), (2150801, 3), (26814387, 3), (27007503, 3), (8821389, 3), (10822574, 3), (19537336, 3), (29868391, 3), (42931572, 3), (45271353, 3), (64467696, 3), (1166428, 3), (1853149, 3), (4743361, 3), (8734046, 3), (13327177, 3), (16966712, 3), (17742072, 3), (16383, 3), (32352129, 3), (36581929, 3), (41648358, 3), (55490096, 3), (61269058, 3), (59707, 3), (168079, 3), (244862, 3), (656933, 3), (1248592, 3), (2996777, 3), (3482503, 3), (3556431, 3), (4723188, 3), (11370, 3), (57240806, 2), (8258172, 2), (51765484, 2), (183628, 2), (22230053, 2), (39812824, 2), (16842834, 2), (33727, 2), (36827305, 1), (22888933, 1), (29384326, 1), (42072639, 1), (63946361, 1)]\n"]}]},{"cell_type":"markdown","source":["## **Testing search_anchor**"],"metadata":{"id":"qwN3_ba8rK4_"}},{"cell_type":"code","source":["search_anchor_query = 'air jordan'\n","search_anchor_result = relevant_anchor_inverted_index.search_anchor(search_anchor_query)\n","print(search_anchor_result)"],"metadata":{"id":"lnyaa7GXrVZz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673283345880,"user_tz":-120,"elapsed":297,"user":{"displayName":"Yuval Schwartz","userId":"10320755735300727416"}},"outputId":"ea921943-e95e-42aa-c51e-0c6f7b6755a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(1394509, 2), (6612, 1), (16556, 1), (19898, 1), (20966, 1), (23146, 1), (23304, 1), (24621, 1), (25679, 1), (26328, 1), (30680, 1), (32090, 1), (37428, 1), (38173, 1), (38957, 1), (39623, 1), (46252, 1), (48563, 1), (52141, 1), (54306, 1), (58422, 1), (63325, 1), (77549, 1), (92313, 1), (107530, 1), (108346, 1), (108646, 1), (140958, 1), (144248, 1), (145623, 1), (148475, 1), (162873, 1), (164036, 1), (185235, 1), (198539, 1), (202594, 1), (202898, 1), (208463, 1), (211583, 1), (246007, 1), (250917, 1), (285974, 1), (289691, 1), (319357, 1), (342775, 1), (357156, 1), (365352, 1), (371568, 1), (377403, 1), (388176, 1), (403355, 1), (418138, 1), (420881, 1), (431130, 1), (431814, 1), (439998, 1), (443038, 1), (483279, 1), (497604, 1), (516142, 1), (551777, 1), (577762, 1), (578183, 1), (583415, 1), (585103, 1), (586049, 1), (613334, 1), (628433, 1), (661186, 1), (680175, 1), (736703, 1), (750263, 1), (754849, 1), (779466, 1), (791422, 1), (898017, 1), (959699, 1), (1021470, 1), (1031040, 1), (1042168, 1), (1054711, 1), (1082976, 1), (1236428, 1), (1236467, 1), (1337980, 1), (1400415, 1), (1485558, 1), (1485799, 1), (1497794, 1), (1515757, 1), (1539546, 1), (1593974, 1), (1657562, 1), (1658301, 1), (1778758, 1), (1789777, 1), (1870519, 1), (1873379, 1), (1895077, 1), (1909983, 1), (1942366, 1), (2163277, 1), (2191770, 1), (2331351, 1), (2452974, 1), (2870802, 1), (3183886, 1), (3657724, 1), (3717944, 1), (4298666, 1), (4314987, 1), (4542150, 1), (4657666, 1), (4878087, 1), (5525855, 1), (5616998, 1), (5676692, 1), (5736421, 1), (6059936, 1), (6077649, 1), (6812364, 1), (6964847, 1), (7168263, 1), (7412236, 1), (8841749, 1), (9213332, 1), (9559286, 1), (10934212, 1), (11534348, 1), (11660526, 1), (11850392, 1), (11996885, 1), (13754313, 1), (14750332, 1), (14998012, 1), (15183570, 1), (16994364, 1), (17092845, 1), (17296107, 1), (17684438, 1), (18047346, 1), (18618063, 1), (19060778, 1), (20148343, 1), (20948989, 1), (21347057, 1), (21576991, 1), (22101487, 1), (23250429, 1), (23250600, 1), (23508196, 1), (23547716, 1), (23860353, 1), (23912768, 1), (24216599, 1), (24218875, 1), (24522124, 1), (24525266, 1), (25014500, 1), (25586593, 1), (26132158, 1), (26457880, 1), (26525690, 1), (30579430, 1), (30687061, 1), (31436142, 1), (32267545, 1), (32843728, 1), (33026935, 1), (35429099, 1), (35833553, 1), (36450985, 1), (37488516, 1), (38645503, 1), (38964192, 1), (39238465, 1), (40858550, 1), (41903764, 1), (41974496, 1), (41974555, 1), (42163310, 1), (42217363, 1), (42473719, 1), (43215703, 1), (44129907, 1), (44732528, 1), (45359871, 1), (46220305, 1), (46265130, 1), (46734540, 1), (46924608, 1), (48122556, 1), (48342835, 1), (48450348, 1), (48507475, 1), (48808889, 1), (53737612, 1), (54182196, 1), (55328338, 1), (55453168, 1), (55594568, 1), (59082207, 1), (59082256, 1), (60051494, 1), (60433194, 1), (60463979, 1), (60476189, 1), (60736300, 1), (61073786, 1), (61711801, 1), (62372638, 1), (63248708, 1), (65259003, 1), (65885677, 1), (66628751, 1), (66902704, 1), (67063926, 1), (67229673, 1), (67229718, 1), (67476964, 1), (67993460, 1), (15717, 1), (20455, 1), (31750, 1), (37966, 1), (53445, 1), (137069, 1), (137072, 1), (150109, 1), (335373, 1), (335854, 1), (480458, 1), (500991, 1), (561662, 1), (598562, 1), (662879, 1), (1067968, 1), (1090065, 1), (1245635, 1), (1311005, 1), (1513732, 1), (1771655, 1), (2310146, 1), (3097723, 1), (3150792, 1), (3845006, 1), (3890370, 1), (4196662, 1), (4374903, 1), (4389874, 1), (7515964, 1), (8821313, 1), (11252389, 1), (11681334, 1), (15416945, 1), (16925321, 1), (18630820, 1), (19765796, 1), (21728262, 1), (22044192, 1), (23006902, 1), (23207990, 1), (23353937, 1), (24223834, 1), (26363308, 1), (27401898, 1), (27453107, 1), (28162045, 1), (30818712, 1), (31083339, 1), (32963694, 1), (33532731, 1), (36740402, 1), (38127431, 1), (40833242, 1), (50066979, 1), (51546226, 1), (54229175, 1), (59373101, 1), (60601430, 1), (66589725, 1), (67838974, 1)]\n"]}]},{"cell_type":"markdown","source":["## **Testing BM25 on body**"],"metadata":{"id":"O5C3_1OfnDj7"}},{"cell_type":"code","source":["search_body_query_string = 'Marijuana'\n","b = 0.75\n","k1 = 1.25\n","k3 = 0\n","search_body_result = relevant_body_inverted_index.search_BM25(search_body_query_string, b, k1, k3, 100)\n","print(search_body_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dsfi4F6OJQqb","executionInfo":{"status":"ok","timestamp":1673282871034,"user_tz":-120,"elapsed":2578,"user":{"displayName":"Yuval Schwartz","userId":"10320755735300727416"}},"outputId":"213f2488-09cc-486b-d018-ace6b4840979"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(19357, 'Marijuana (disambiguation)', 0.7229397096202594), (28985374, 'Medical cannabis card', 0.3799334984837808), (1227367, 'British Columbia Marijuana Party', 0.28216643778233313), (53836251, 'List of cannabis rights organizations', 0.28113152365015565), (19920359, 'Marijuana (word)', 0.21291740779565077), (52209782, 'Cannabis in Hawaii', 0.205314940994269), (383537, 'Marijuana Party (Canada)', 0.19983619331503555), (52356241, 'Cannabis in Thailand', 0.15547708592186352), (48920848, 'Cannabis in Alaska', 0.10831512131944637), (28572685, 'Cannabis in Malawi', 0.09581352613689148), (184488, 'National Organization for the Reform of Marijuana Laws', 0.09269725432961426), (44975261, 'Medical Marijuana, Inc.', 0.08495610888774935), (52184272, 'Cannabis in Minnesota', 0.07526340131299288), (20566488, 'Cannabis in the United States', 0.0625486674680209), (53871120, 'List of names for cannabis', 0.051890922831448416), (48640150, 'Minors and the legality of cannabis', 0.05133338915300779), (52228042, 'Cannabis in Ohio', 0.03944384908226114), (19760623, 'Joint (cannabis)', 0.03845273361321535), (145891, '420 (cannabis culture)', 0.03843961306448326), (31188467, 'Legality of cannabis by U.S. jurisdiction', 0.03437797058935597), (52227830, 'Cannabis in Florida', 0.027285919268526516), (20481920, 'Cannabis strain', 0.026853757683914875), (14942276, 'Cannabis culture', 0.026645661529807005), (53897655, 'List of cannabis seizures', 0.02365472020305955), (52183794, 'Cannabis in Missouri', 0.021109557893327587), (2331004, 'Cannabis and religion', 0.019757101860382684), (56078060, 'History of cannabis', 0.018302333182940965), (52342272, 'Cannabis in Israel', 0.01821833338230229), (37646421, 'Timeline of cannabis laws in the United States', 0.017354692878721847), (8596369, 'Legalization of non-medical cannabis in the United States', 0.016218022224079115), (3045683, 'Cannabis in Canada', 0.015532455105734757), (168915, 'Effects of cannabis', 0.014976880580320855), (1910732, 'Newport (cigarette)', 0.014375987532104404), (49131135, 'Lawnmower Dog', 0.01125731848529108), (4512923, 'Cannabis smoking', 0.01031107502245274), (23154203, 'Hash oil', 0.010125016996804785), (1481886, 'Cannabis (drug)', 0.009971823266755692), (175440, 'Medical cannabis', 0.009383942264438744), (150113, 'Cannabis edible', 0.009250147268352845), (60920, 'Tetrahydrocannabinol', 0.007995353834328457), (68188835, 'Cannabis and sports', 0.007396729167102401), (11164587, 'Legal history of cannabis in the United States', 0.007068816148634696), (20866399, 'Synthetic cannabinoids', 0.0059965717678631516), (27202445, 'Cannabis use disorder', 0.005988046607104107), (615418, 'Kids (film)', 0.005618659736000007), (168917, 'Legality of cannabis', 0.00347375697877747), (22707918, 'Medical cannabis in the United States', 0.003363519568419015), (47227709, 'Cannabis in India', 0.0027823652106020065), (38310, 'Cannabis', 0.001485273487829323), (51237650, 'History of Tesla, Inc.', 0.0006069265945413486), (43345713, 'Killing of Eric Garner', 0.000591441451293637), (32706, 'Vancouver', 0.000505344544421075), (50276542, 'Criticism of Netflix', 0.0003918227333803123), (1625137, 'Simpson family', 0.0003821520218589613), (13603, 'Homeschooling', 0.0003627335806606477), (8522, 'Denver', 0.00026816029646600374), (73298, 'Tobacco smoking', 0.00022744131660132344), (11996885, 'Electronic cigarette', 0.0001007963837868204), (292279, 'List of recurring The Simpsons characters', 1.537407281269485e-05)]\n"]}]},{"cell_type":"code","source":["search_body_query_string = 'marijuana'\n","b = 0.75\n","k1 = 1.25\n","k3 = 0\n","search_body_result = relevant_body_inverted_index.search_BM25(search_body_query_string, b, k1, k3, 100)\n","print(search_body_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJkGCF7JLe-J","executionInfo":{"status":"ok","timestamp":1673536515928,"user_tz":-120,"elapsed":466,"user":{"displayName":"Amit Kravchik","userId":"18309304432468869556"}},"outputId":"7f914443-358c-4473-f5a8-0e5fbc750a97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(19357, 'Marijuana (disambiguation)', 0.7229397096202594), (28985374, 'Medical cannabis card', 0.37993349848378083), (1227367, 'British Columbia Marijuana Party', 0.28216643778233325), (53836251, 'List of cannabis rights organizations', 0.28113152365015565), (19920359, 'Marijuana (word)', 0.21291740779565085), (52209782, 'Cannabis in Hawaii', 0.20531494099426903), (383537, 'Marijuana Party (Canada)', 0.1998361933150356), (52356241, 'Cannabis in Thailand', 0.15547708592186352), (48920848, 'Cannabis in Alaska', 0.1083151213194464), (28572685, 'Cannabis in Malawi', 0.0958135261368915), (184488, 'National Organization for the Reform of Marijuana Laws', 0.09269725432961429), (44975261, 'Medical Marijuana, Inc.', 0.08495610888774936), (52184272, 'Cannabis in Minnesota', 0.07526340131299289), (20566488, 'Cannabis in the United States', 0.06254866746802093), (53871120, 'List of names for cannabis', 0.05189092283144842), (48640150, 'Minors and the legality of cannabis', 0.0513333891530078), (52228042, 'Cannabis in Ohio', 0.03944384908226116), (19760623, 'Joint (cannabis)', 0.03845273361321536), (145891, '420 (cannabis culture)', 0.038439613064483284), (31188467, 'Legality of cannabis by U.S. jurisdiction', 0.034377970589355995), (52227830, 'Cannabis in Florida', 0.027285919268526523), (20481920, 'Cannabis strain', 0.026853757683914885), (14942276, 'Cannabis culture', 0.02664566152980702), (53897655, 'List of cannabis seizures', 0.02365472020305956), (52183794, 'Cannabis in Missouri', 0.021109557893327597), (2331004, 'Cannabis and religion', 0.01975710186038269), (56078060, 'History of cannabis', 0.01830233318294097), (52342272, 'Cannabis in Israel', 0.018218333382302297), (37646421, 'Timeline of cannabis laws in the United States', 0.017354692878721857), (8596369, 'Legalization of non-medical cannabis in the United States', 0.016218022224079125), (3045683, 'Cannabis in Canada', 0.01553245510573476), (168915, 'Effects of cannabis', 0.014976880580320862), (1910732, 'Newport (cigarette)', 0.014375987532104407), (49131135, 'Lawnmower Dog', 0.011257318485291082), (4512923, 'Cannabis smoking', 0.010311075022452744), (23154203, 'Hash oil', 0.01012501699680479), (1481886, 'Cannabis (drug)', 0.009971823266755697), (175440, 'Medical cannabis', 0.009383942264438753), (150113, 'Cannabis edible', 0.009250147268352852), (60920, 'Tetrahydrocannabinol', 0.007995353834328462), (68188835, 'Cannabis and sports', 0.007396729167102403), (11164587, 'Legal history of cannabis in the United States', 0.0070688161486347), (20866399, 'Synthetic cannabinoids', 0.005996571767863155), (27202445, 'Cannabis use disorder', 0.005988046607104109), (615418, 'Kids (film)', 0.005618659736000009), (168917, 'Legality of cannabis', 0.0034737569787774716), (22707918, 'Medical cannabis in the United States', 0.0033635195684190173), (47227709, 'Cannabis in India', 0.002782365210602008), (38310, 'Cannabis', 0.0014852734878293239), (51237650, 'History of Tesla, Inc.', 0.0006069265945413488), (43345713, 'Killing of Eric Garner', 0.0005914414512936373), (32706, 'Vancouver', 0.0005053445444210754), (50276542, 'Criticism of Netflix', 0.00039182273338031254), (1625137, 'Simpson family', 0.00038215202185896155), (13603, 'Homeschooling', 0.00036273358066064794), (8522, 'Denver', 0.00026816029646600396), (73298, 'Tobacco smoking', 0.0002274413166013236), (11996885, 'Electronic cigarette', 0.00010079638378682047), (292279, 'List of recurring The Simpsons characters', 1.5374072812694858e-05)]\n"]}]},{"cell_type":"markdown","source":["# **Page Rank & Page View**"],"metadata":{"id":"J6_wzKSuEnd-"}},{"cell_type":"code","source":["# with open(PAGE_RANKS_PATH, 'rb') as f:\n","#   doc_id_to_pagerank_dict = pickle.loads(f.read())"],"metadata":{"id":"nL7h-3PZrik1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_pagerank():\n","    ''' Returns PageRank values for a list of provided wiki article IDs. \n","\n","        Test this by issuing a POST request to a URL like:\n","          http://YOUR_SERVER_DOMAIN/get_pagerank\n","        with a json payload of the list of article ids. In python do:\n","          import requests\n","          requests.post('http://YOUR_SERVER_DOMAIN/get_pagerank', json=[1,5,8])\n","        As before YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n","        if you're using ngrok on Colab or your external IP on GCP.\n","    Returns:\n","    --------\n","        list of floats:\n","          list of PageRank scores that correrspond to the provided article IDs.\n","    '''\n","    res = []\n","    wiki_ids = request.get_json()\n","    if len(wiki_ids) == 0:\n","      return jsonify(res)\n","    # BEGIN SOLUTION\n","    res = [doc_id_to_pagerank_dict[wiki_id] for wiki_id in wiki_ids]\n","    # END SOLUTION\n","    return jsonify(res)"],"metadata":{"id":"htzG_OUQZeVF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# with open(PAGE_VIEWS_PATH, 'rb') as f:\n","#   doc_id_to_pageview_dict = pickle.loads(f.read())"],"metadata":{"id":"jTPtT4zb-HYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_pageview():\n","    ''' Returns the number of page views that each of the provide wiki articles\n","        had in August 2021.\n","\n","        Test this by issuing a POST request to a URL like:\n","          http://YOUR_SERVER_DOMAIN/get_pageview\n","        with a json payload of the list of article ids. In python do:\n","          import requests\n","          requests.post('http://YOUR_SERVER_DOMAIN/get_pageview', json=[1,5,8])\n","        As before YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n","        if you're using ngrok on Colab or your external IP on GCP.\n","    Returns:\n","    --------\n","        list of ints:\n","          list of page view numbers from August 2021 that correrspond to the \n","          provided list article IDs.\n","    '''\n","    res = []\n","    wiki_ids = request.get_json()\n","    if len(wiki_ids) == 0:\n","      return jsonify(res)\n","    # BEGIN SOLUTION\n","    res = [doc_id_to_pageview_dict[wiki_id] for wiki_id in wiki_ids]\n","    # END SOLUTION\n","    return jsonify(res)"],"metadata":{"id":"yPZ1nZW7sCCz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Generating different IRs**"],"metadata":{"id":"tfMJNQioA387"}},{"cell_type":"code","source":["def search_over_body(query_to_search, cosine=False, BM25=False):\n","  if cosine:\n","    return relevant_body_inverted_index.calculate_cossim_by_weights_of_tfidf(query_to_search)\n","  if BM25:\n","    return relevant_body_inverted_index.generate_doc_id_to_BM25_score_rdd(query_to_search)"],"metadata":{"id":"hUHYgOUbKk22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def search_over_title(query_to_search, cosine=False, BM25=False, boolean=False):\n","  if cosine:\n","    return relevant_title_inverted_index.calculate_cossim_by_weights_of_tfidf(query_to_search)\n","  if BM25:\n","    return relevant_title_inverted_index.generate_doc_id_to_BM25_score_rdd(query_to_search)\n","  if boolean:\n","    return relevant_title_inverted_index.rank_by_boolean_scores(query_to_search)\n","  "],"metadata":{"id":"52r-RWgHZkuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def search_over_anchor(query_to_search, cosine=False, BM25=False, boolean=False):\n","  pass"],"metadata":{"id":"GVHzDF1jZncm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def search(query_to_search, func_title, w_title, func_body ,w_body, func_anchor, w_anchor, w_pagerank):\n","  title_scores_rdd = search_over_title(query_to_search, *func_title)\n","  body_scores_rdd = search_over_body(query_to_search, *func_body)\n","  anchor_scores_rdd = search_over_anchor(query_to_search, *func_anchor)\n","\n","  combination_rdd = title_scores_rdd.union(body_scores_rdd).union(anchor_scores_rdd)\n","  weighted_title_body_anchor_score_rdd = combination_rdd.map(lambda doc_id_to_scores_list: (doc_id_to_scores_list[0], (w_title * doc_id_to_scores_list[1][0]) + (w_body * doc_id_to_scores_list[1][1]) + (w_anchor * doc_id_to_scores_list[1][2])))\n","  weighted_score_with_pagerank_rdd = weighted_title_body_anchor_score_rdd.map(lambda doc_id_to_score: (doc_id_to_score[0], (w_pagerank * get_pagerank(doc_id_to_score[0])) + ((1 - w_pagerank) * doc_id_to_score[1])))\n","  "],"metadata":{"id":"7GRx3lMYaEJT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_weights_and_search(query_to_search, offset=0.1):\n","  results_dict = {}\n","  for b in range(0, 11):\n","    for t in range(0, 11):\n","        for a in range(0, 11):\n","            for pr in range(0, 11):\n","                if b + t + a + pr == 10:\n","                    weights = (b/10, t/10, a/10, pr/10)\n","                    results_dict[weights] = search(query_to_search, *weights)"],"metadata":{"id":"di5MbQhfjFr7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Recall & Precision**"],"metadata":{"id":"_so4fBu2gHp7"}},{"cell_type":"code","source":["nirs_queries_to_results_first_20 = {\"best marvel movie\": [60283633, 61073786, 5676692, 56289553, 60774345, 27306717, 61592102, 42163310, 60952488, 36439749, 48530084, 10589717, 29129051, 59892, 612052, 44254295, 878659, 54653881, 51430647, 66111204, 22114132, 55935213, 41677925, 17296107, 61651800, 9110929, 67055, 37497391, 60744481, 65464184, 41974496, 60616450, 60463979, 65967176, 57069491, 46208997, 22144990, 62372638, 1074657, 44240443, 33463661, 41974555, 43603241, 33700618, 5027882, 66423851, 60754840],\\\n","                                    \"How do kids come to world?\": [15474, 1357127, 636806, 43033258, 6271835, 56480301, 23133297, 615418, 73165, 24470328, 1833777, 1380383, 79449, 4827661, 387703, 18863597, 36827305, 494299, 194687, 5591344, 48490547, 634139, 42072639, 44311171, 29384326, 1908019, 296627, 11263877, 101942, 2045465, 56921904, 128987, 22888933, 1072968, 25490788, 83449, 884998, 1151454, 30640885, 35072597, 2535885, 30861, 51046955, 13603, 3060346, 88380, 19698110, 72214, 6236554, 46105],\\\n","                                    \"Information retrieval\": [1897206, 10179411, 25130414, 5818361, 1185840, 20948989, 48317971, 509628, 494528, 11486091, 50716473, 24963841, 296950, 35804330, 261193, 15271, 39000674, 19988623, 38156944, 36794719, 731640, 14109784, 10328235, 25935906, 16635934, 33407925, 743971, 3781784, 14343887, 57312392, 24997830, 442684, 7872152, 14473878, 25959000, 9511414],\\\n","                                    \"LinkedIn\": [3591502, 55679006, 970755, 36070366, 63641225, 41726116, 51562019, 35549457, 21179478, 62976368, 27769500, 57147095, 31403505, 22291643, 50191962],\\\n","                                    \"How to make coffee?\": [4506407, 321546, 37249793, 17668101, 26731675, 6887661, 1566948, 5612891, 211895, 68117784, 4604645, 47660, 3757402, 273707, 8866584, 5964683, 49099835, 28890200, 53151326, 300805, 1623162, 3775558, 273700, 667037, 5212064, 6826364, 63534797, 54459918, 604727, 30860428, 2461806, 3639440, 2929216, 12343966, 408360, 63520964, 838057, 6332026, 19619306, 215424, 482824, 38579961, 8728856, 2165666, 3785715, 366244, 1646753, 31824340],\\\n","                                    \"Ritalin\": [649100, 8802530, 6428730, 608718, 13594085, 66391, 25164479, 24754461, 22611786, 964614, 7432624, 5721484, 57068567, 1333695, 4387617, 463961, 23891416, 56961277, 47956615, 4726434, 52780757, 50762105, 40542151, 1186041, 10671710, 7594242, 57762, 2580091, 159284, 2495940, 6281833, 45690249, 1546447, 32325617, 205878, 1790029, 5497377],\\\n","                                    \"How to make wine at home?\": [373172, 3602925, 20790067, 223834, 15468138, 3398365, 61014433, 19600890, 927688, 146918, 22216378, 1417287, 13824744, 57098, 3276784, 466664, 41337483, 1031040, 36029170, 29324283, 26924822, 31505523, 13532634, 4378282, 1045027, 1455948, 14825456, 485220, 37468361, 1041458, 8177057, 2866516, 31704630, 21991369, 4554556, 713636, 8608425, 20810258, 22777652, 1039412, 32961, 8778890, 683094, 19561784, 6032951, 10998, 5222577, 7414829, 20185928, 8318345],\\\n","                                    \"Most expensive city in the world\": [33508970, 3602421, 94167, 24724090, 30057, 220886, 31453, 19058, 31326350, 32706, 645042, 3928523, 18402, 34374079, 522934, 13476079, 2376810, 36511, 172538, 15218891, 390875, 22309, 12521, 65708464, 1664254, 35368654, 19004, 309890, 27862, 27318, 45470, 10992, 53446, 19261, 19189, 3848717, 11947794, 49749249, 7780, 14900757, 9299090, 26976, 49728, 63946361, 302201],\\\n","                                    \"India\": [141896, 14745, 24452, 265059, 14597, 13890, 42737, 2377, 1186115, 6825785, 26457880, 1472206, 17359901, 37756, 53707, 315776, 4208015, 295335, 14598, 1996872, 764545, 1108803, 3574003, 678583, 7564733, 37534, 2198463, 720414, 6622547, 1683930, 231623, 17774253, 14533, 19189, 275047, 20611562, 43281, 17719886, 10710364, 5864614, 3315459, 14580, 47905, 3799826, 553883, 375986, 408215],\\\n","                                    \"how to make money fast?\": [67987778, 12789839, 5624681, 44379765, 400777, 47720307, 45332, 1531043, 48732, 7322279, 51895777, 65228, 60739751, 21175589, 846772, 9833167, 22226313, 63809606, 35666788, 1527716, 4416646, 23830729, 264058, 32595633, 1335238, 12020461, 1793651, 1370831, 63121, 2913859, 42994, 4090453, 17418777, 5145001, 43250171, 8957449, 43030666, 473309, 624998, 7555986, 22156522, 13681, 29681566, 17362858, 19390, 407288, 1276547, 2763667],\\\n","                                    \"Netflix\": [65595607, 34075129, 50602056, 65741484, 32670973, 61972257, 66174045, 47048067, 49016960, 63732884, 175537, 56312051, 65073808, 59629338, 54671372, 56312054, 50276542, 57041239, 66422422, 67450679, 66299065, 9399111, 50137861, 40030145],\\\n","                                    \"Apple computer\": [254496, 50865995, 5285468, 5653238, 3356874, 345676, 2275, 4478297, 2593693, 3608414, 18640, 248101, 15183570, 20647724, 1159939, 17826747, 619983, 856, 46728817, 2116, 1492625, 77118, 32327247, 15357987, 400593, 17997437, 1005263, 345354, 2020710, 660310, 1344, 19006979, 15295713, 2786155, 2117, 21694, 233780, 5078775, 73262, 21347643, 27848, 548115],\\\n","                                    \"The Simpsons\": [19293758, 1424178, 74813, 1625137, 34519668, 4939408, 11028525, 49387265, 4939471, 292279, 60534017, 9306179, 33350134, 4939519, 1466966, 4939306, 4939444, 140332, 4939501, 29838, 5451605, 19266557, 3038969, 14040227, 4939334, 188572, 10765975, 22423628, 4776930],\\\n","                                    \"World cup\": [32516422, 42931572, 2996777, 33727, 183628, 60637832, 8821389, 16842834, 22230053, 1166428, 29868391, 64467696, 4743361, 13327177, 61269058, 26814387, 62528055, 10822574, 3482503, 36581929, 8258172, 16966712, 39302261, 244862, 67608822, 1853149, 39812824, 55490096, 2150801, 8734046, 32352129, 16383, 59707, 19537336, 3556431, 17742072, 11370, 656933, 168079, 41648358, 4723188, 1248592],\\\n","                                    \"How to lose weight?\": [400199, 1151047, 791546, 67730903, 27300359, 84252, 26639763, 8581665, 1148926, 64543917, 6319249, 2029766, 56885915, 11665493, 1958879, 28396636, 56435, 2883760, 31429041, 32051848, 277790, 11884255, 49051658, 1017976, 42528947, 1149933, 65004286, 4748844, 44442017, 35281209, 40925771, 30687447, 11249433, 45280337, 17659030, 8460, 3549164, 727293, 28541957, 12523816, 33825347, 18168862, 9972157, 410007, 27148738],\\\n","                                    \"Java\": [1179384, 17521476, 5516020, 5863400, 15628, 4093054, 135063, 663788, 9845, 1455590, 3901428, 731735, 1079500, 24920873, 11125049, 7955681, 38321273, 456722, 15881, 16389, 26257672, 43284, 651278, 127604, 43826, 314356, 53078721, 611589, 1131136, 230828, 417018, 42870, 69336, 4718446, 1414212, 7811267, 42871, 40659966, 13593, 1326984, 453584, 320443, 30120784, 7771171, 269441, 4294832],\\\n","                                    \"Air Jordan\": [3647739, 3890370, 6722408, 105344, 18998781, 1371219, 60601430, 7851893, 28155315, 1394509, 4253801, 36916362, 265033, 23353937, 13365219, 20455, 3097723, 50066979, 51546226, 2310146, 67838974, 9998569, 62741501, 58209447],\\\n","                                    \"how to deal with depression?\": [2721889, 13190302, 63499429, 16360289, 39218436, 33310173, 2367697, 57688, 20529621, 4041101, 49233423, 2685269, 840273, 25258288, 43600438, 60611538, 19283335, 18550003, 33255495, 19356, 60457349, 2891701, 66811, 34753948, 43875835, 42730418, 717119, 1295947, 18176448, 2353519, 1879108, 14325087, 3440273, 175357, 16407460, 3762294, 4531, 19064282, 52316, 8389, 255475, 341658, 20448627, 22481627, 21211994, 5144613, 30846934, 1500618, 234796],\\\n","                                    \"How do you make gold\": [323246, 5580137, 1686492, 1385632, 23290471, 6890967, 15739, 39740796, 62929, 1020809, 251087, 6109962, 6996576, 402244, 2015573, 20063724, 1230653, 180211, 7133952, 23324, 12240, 1291393, 3519942, 12095348, 44712684, 27119, 886856, 18300514, 25918508, 37412, 2526649, 39639653, 390698, 1356272, 10865561, 1386629, 5024105, 3706246, 67110306, 2732267, 15457257, 56226, 19074264, 63280480, 1581831, 45756, 2927992, 27345986, 152176],\\\n","                                    \"Marijuana\": [60920, 52227830, 22707918, 4512923, 68188835, 28985374, 31188467, 52184272, 52209782, 27202445, 20481920, 1481886, 19920359, 2331004, 19357, 44975261, 145891, 28572685, 20566488, 37646421, 383537, 20866399, 53836251, 150113, 53871120, 19760623, 3045683, 8596369, 1227367, 168917, 14942276, 48640150, 52342272, 52356241, 56078060, 38310, 175440, 53897655, 52228042, 52183794, 11164587, 168915, 48920848, 47227709, 23154203, 184488]}"],"metadata":{"id":"gpkN-bHCgNGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_map_40_over_nirs_first_20_queries(title=False, body=False, anchor=False, cossim=False, BM25=False):\n","\n","  if title:\n","    inverted_index = relevant_title_inverted_index\n","  elif body:\n","    inverted_index = relevant_body_inverted_index\n","  else:\n","    inverted_index = relevant_anchor_inverted_index\n","\n","  sum_of_average_precisions = 0\n","  for query, results_list in nirs_queries_to_results_first_20.items():\n","    if cossim:\n","      top_40_results = inverted_index.search_cossim(query, 40)\n","    else:\n","      top_40_results = inverted_index.search_BM25(query, 0.75, 1.25, 0, 40)\n","\n","    num_of_hits = 0\n","    sum_of_precisions = 0\n","    for index, doc_id, doc_title, doc_score in [(i+1, *elements) for i, elements in enumerate(top_40_results)]:\n","      if doc_id in results_list:\n","        num_of_hits += 1\n","        precision = num_of_hits / index\n","        sum_of_precisions += precision\n","    average_precision = sum_of_precisions / num_of_hits\n","    print('Average Precision for the query ',query , average_precision)\n","    sum_of_average_precisions += average_precision\n","  return sum_of_average_precisions / 20"],"metadata":{"id":"wJh1xxL5gdrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["body_BM25_map_40_result = calculate_map_40_over_nirs_first_20_queries(body=True, BM25=True)\n","print()\n","print(body_BM25_map_40_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AUGIunYiR1MO","executionInfo":{"status":"ok","timestamp":1673448086632,"user_tz":-120,"elapsed":298,"user":{"displayName":"Amit Kravchik","userId":"18309304432468869556"}},"outputId":"07f2cde0-5455-41d1-8cf9-d7af34ccb768"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Precision for the query  best marvel movie 0.6901217150676671\n","Average Precision for the query  How do kids come to world? 0.6001606978879704\n","Average Precision for the query  Information retrieval 0.9858512594764861\n","Average Precision for the query  LinkedIn 1.0\n","Average Precision for the query  How to make coffee? 1.0\n","Average Precision for the query  Ritalin 0.9847558201901625\n","Average Precision for the query  How to make wine at home? 0.9531142810529131\n","Average Precision for the query  Most expensive city in the world 0.3939060200774049\n","Average Precision for the query  India 0.7474784648030819\n","Average Precision for the query  how to make money fast? 0.9464357751028148\n","Average Precision for the query  Netflix 0.7517846715607228\n","Average Precision for the query  Apple computer 0.8712914157239983\n","Average Precision for the query  The Simpsons 0.9522760754595276\n","Average Precision for the query  World cup 0.655255453245081\n","Average Precision for the query  How to lose weight? 0.9712721340622639\n","Average Precision for the query  Java 1.0\n","Average Precision for the query  Air Jordan 0.9154519354519355\n","Average Precision for the query  how to deal with depression? 0.9836117069005742\n","Average Precision for the query  How do you make gold 0.9978075515059868\n","Average Precision for the query  Marijuana 0.9915614185004766\n","\n","0.8696068198034534\n"]}]},{"cell_type":"code","source":["body_BM25_map_40_result = calculate_map_40_over_nirs_first_20_queries(body=True, BM25=True)\n","print()\n","print(body_BM25_map_40_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R81D3Ry2rMl3","executionInfo":{"status":"ok","timestamp":1673283188615,"user_tz":-120,"elapsed":38423,"user":{"displayName":"Yuval Schwartz","userId":"10320755735300727416"}},"outputId":"3c2a4c68-9612-4ee7-a919-85aff66e6f53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Precision for the query  best marvel movie 0.6901217150676671\n","Average Precision for the query  How do kids come to world? 0.6001606978879704\n","Average Precision for the query  Information retrieval 0.9858512594764861\n","Average Precision for the query  LinkedIn 1.0\n","Average Precision for the query  How to make coffee? 1.0\n","Average Precision for the query  Ritalin 0.9847558201901625\n","Average Precision for the query  How to make wine at home? 0.9531142810529131\n","Average Precision for the query  Most expensive city in the world 0.3939060200774049\n","Average Precision for the query  India 0.7474784648030819\n","Average Precision for the query  how to make money fast? 0.9464357751028148\n","Average Precision for the query  Netflix 0.7517846715607228\n","Average Precision for the query  Apple computer 0.8712914157239983\n","Average Precision for the query  The Simpsons 0.9522760754595276\n","Average Precision for the query  World cup 0.655255453245081\n","Average Precision for the query  How to lose weight? 0.9712721340622639\n","Average Precision for the query  Java 1.0\n","Average Precision for the query  Air Jordan 0.9154519354519355\n","Average Precision for the query  how to deal with depression? 0.9836117069005742\n","Average Precision for the query  How do you make gold 0.9978075515059868\n","Average Precision for the query  Marijuana 0.9915614185004766\n","\n","0.8696068198034534\n"]}]},{"cell_type":"code","source":["body_cossim_map_40_result = calculate_map_40_over_nirs_first_20_queries(body=True, cossim=True)\n","print()\n","print(body_cossim_map_40_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1jijj4JR6c7","executionInfo":{"status":"ok","timestamp":1673448108308,"user_tz":-120,"elapsed":257,"user":{"displayName":"Amit Kravchik","userId":"18309304432468869556"}},"outputId":"94a70873-f3fa-4d10-c9fc-15383429dea9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Precision for the query  best marvel movie 0.9677265206651526\n","Average Precision for the query  How do kids come to world? 0.8300897606109894\n","Average Precision for the query  Information retrieval 0.9927625751155161\n","Average Precision for the query  LinkedIn 1.0\n","Average Precision for the query  How to make coffee? 1.0\n","Average Precision for the query  Ritalin 0.9922829421909688\n","Average Precision for the query  How to make wine at home? 1.0\n","Average Precision for the query  Most expensive city in the world 0.6202262278270324\n","Average Precision for the query  India 0.9993589743589744\n","Average Precision for the query  how to make money fast? 0.9793869243773587\n","Average Precision for the query  Netflix 0.8708298974906481\n","Average Precision for the query  Apple computer 1.0\n","Average Precision for the query  The Simpsons 0.9942449282271306\n","Average Precision for the query  World cup 0.9054325595965992\n","Average Precision for the query  How to lose weight? 0.9979214778383753\n","Average Precision for the query  Java 1.0\n","Average Precision for the query  Air Jordan 0.9643061105054938\n","Average Precision for the query  how to deal with depression? 0.9898303878935982\n","Average Precision for the query  How do you make gold 1.0\n","Average Precision for the query  Marijuana 1.0\n","\n","0.9552199643348919\n"]}]},{"cell_type":"code","source":["body_cossim_map_40_result = calculate_map_40_over_nirs_first_20_queries(body=True, cossim=True)\n","print()\n","print(body_cossim_map_40_result)"],"metadata":{"id":"Ulv38rV6G8b2"},"execution_count":null,"outputs":[]}]}